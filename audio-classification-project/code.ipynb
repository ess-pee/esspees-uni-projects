{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Short Project \n",
    "#### Author - Shaurya Pathak. \n",
    "\n",
    "This script implements a basic 2d convolutional neural network classifier which finds the best parameters for number of layers and number of filters through a random search algorithm, after which it uses ensemble averaging which involves a combination of neural networks training on subsets of data and voting on the class. Finally this script implements bayesian optimisation using the hyperopt library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, Activation, Input, Conv2D, MaxPooling2D, Flatten, Softmax\n",
    "from keras import optimizers, regularizers\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "import keras\n",
    "import random\n",
    "from hyperopt import fmin, tpe, hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a neural network class that is hard coded to work for this assignment\n",
    "class nn:\n",
    "    def __init__(self, n_classes, n_layers, n_filters, filter_size, stride_length, d_prob, epsilon):\n",
    "\n",
    "        '''\n",
    "        Instantiates a sequential neural network based on the input parameters, it is also hard coded to accommodate inputs from this assignment assuming input size of (98,50), pool-size of (2,2), relu activation, same padding, adam optimiser and categorical-cross-entropy loss\n",
    "        \n",
    "        Args: \n",
    "            n_classes (int): The number of classes\n",
    "            n_layers (int): The number of layers, in this context a layer is a convolutional layer + batch normalisation layer + activation + max pool. This does not include the input layer or the dense softmax layer.\n",
    "            n_filters (int): The number of filters added to each convolutional layer\n",
    "            filter_size (tuple (int)): The size of each filter. Should be in format (i, j), i.e. a tuple\n",
    "            stride_length (tuple (int)): The rate at which each filter will convolve over the input. Should be in format (i, j), i.e. a tuple\n",
    "            d_prob (float .00): The dropout probability of the dropout layer. Ideally between (0.00:0.30)\n",
    "            epsilon (float .000): The learning rate of the model\n",
    "        '''\n",
    "        # Initialise the model\n",
    "        self.model = Sequential()\n",
    "        \n",
    "        # Input Layer\n",
    "        self.model.add(Input(shape=(98,50,1)))\n",
    "\n",
    "        # Convolutional Layers: nothing special here just skips adding a pooling layer at whatever the last convolutional layer is to accommdate for the dropout\n",
    "        for i in range(n_layers):\n",
    "            self.model.add(Conv2D(n_filters, kernel_size = filter_size, padding='same'))\n",
    "            self.model.add(BatchNormalization())\n",
    "            self.model.add(Activation('relu'))\n",
    "            if i != n_layers-1:\n",
    "                self.model.add(MaxPooling2D(pool_size =(2, 2), strides=stride_length, padding= 'same'))\n",
    "        \n",
    "        self.model.add(Dropout((d_prob)))\n",
    "        self.model.add(MaxPooling2D(pool_size =(12, 1), strides=(1, 1), padding = 'same'))\n",
    "        # Final Layer\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(n_classes))\n",
    "        self.model.add(Softmax())\n",
    "\n",
    "        # Set the optimization options and compile the model\n",
    "        opt = optimizers.Adam(learning_rate=epsilon)\n",
    "        self.model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "        \n",
    "    def fit(self, x_train, y_train):\n",
    "        ''' \n",
    "        Fits the designed model as it was first initialised\n",
    "        '''\n",
    "        callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=0)\n",
    "        self.history = self.model.fit(x_train, y_train, batch_size=16, epochs=20, validation_split=0.2, callbacks=callback, verbose=0)\n",
    "\n",
    "    def evaluate(self, x_val, y_val):\n",
    "        '''\n",
    "        Stores the keras evaluation score list and also prints the validation accuracy as a percentage\n",
    "        '''\n",
    "        self.score = self.model.evaluate(x_val, y_val, verbose=0)\n",
    "        print([f'Validation Accuracy: {np.round(self.score[1]*100, 2)}%'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2001 files belonging to 12 classes.\n",
      "Found 1171 files belonging to 12 classes.\n"
     ]
    }
   ],
   "source": [
    "# Image dataset loading code\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory='speechImageData/TrainData',\n",
    "    labels='inferred',\n",
    "    color_mode=\"grayscale\",\n",
    "    label_mode='categorical',\n",
    "    batch_size=128,\n",
    "    image_size=(98, 50)\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory='speechImageData/ValData',\n",
    "    labels='inferred',\n",
    "    color_mode=\"grayscale\",\n",
    "    label_mode='categorical',\n",
    "    batch_size=128,\n",
    "    image_size=(98, 50)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the  training input images and output class labels\n",
    "x_train = []\n",
    "y_train = []\n",
    "for images, labels in train_ds.take(-1):\n",
    "    x_train.append(images.numpy())\n",
    "    y_train.append(labels.numpy())\n",
    "\n",
    "x_train = np.concatenate(x_train, axis=0)\n",
    "y_train = np.concatenate(y_train, axis=0)\n",
    "\n",
    "# Extract the validation input images and output class labels\n",
    "x_val = []\n",
    "y_val = []\n",
    "for images, labels in val_ds.take(-1):\n",
    "    x_val.append(images.numpy())\n",
    "    y_val.append(labels.numpy())\n",
    "\n",
    "x_val = np.concatenate(x_val, axis=0)\n",
    "y_val = np.concatenate(y_val, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1171, 12)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers: 6 filters:  1\n",
      "['Validation Accuracy: 14.18%']\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 30\u001b[0m\n\u001b[0;32m     21\u001b[0m model \u001b[38;5;241m=\u001b[39m nn(n_classes\u001b[38;5;241m=\u001b[39m y_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \n\u001b[0;32m     22\u001b[0m                     n_layers\u001b[38;5;241m=\u001b[39m l,\n\u001b[0;32m     23\u001b[0m                     n_filters\u001b[38;5;241m=\u001b[39m f,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m                     d_prob\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m,\n\u001b[0;32m     27\u001b[0m                     epsilon\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Printing our found results\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayers:\u001b[39m\u001b[38;5;124m'\u001b[39m, l,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilters: \u001b[39m\u001b[38;5;124m'\u001b[39m, f)\n\u001b[0;32m     32\u001b[0m model\u001b[38;5;241m.\u001b[39mevaluate(x_val, y_val)\n",
      "Cell \u001b[1;32mIn[2], line 47\u001b[0m, in \u001b[0;36mnn.fit\u001b[1;34m(self, x_train, y_train)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m''' \u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;124;03mFits the designed model as it was first initialised\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     46\u001b[0m callback \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# This cell performs the random search algorithm, by randomly selecting points from two pre-defined arrays containing hyperparameters for number of layers and number of filters \n",
    "\n",
    "# Seeding for reproducability\n",
    "np.random.seed(0)\n",
    "\n",
    "# Defining the number of random points to investigate variable\n",
    "rndm_pts = 9\n",
    "\n",
    "# These two just create the range of hyperparameters our for loop is going to use while randomly sampling\n",
    "layers = np.arange(1, 10)\n",
    "filters = np.power(2, np.arange(9))\n",
    "\n",
    "# At this stage I don't save the instantiated models in a list because we're just not interested in that yet, just the final performance\n",
    "for i in range(rndm_pts):\n",
    "\n",
    "    # Randomly selecting hyperparameters from the structures defined outside of the loop\n",
    "    l = np.random.choice(layers)\n",
    "    f = np.random.choice(filters)\n",
    "\n",
    "    # Instantiating a temporary model\n",
    "    model = nn(n_classes= y_train.shape[1], \n",
    "                        n_layers= l,\n",
    "                        n_filters= f,\n",
    "                        filter_size= (3,3),\n",
    "                        stride_length= (2,2),\n",
    "                        d_prob= 0.2,\n",
    "                        epsilon= 0.001)\n",
    "    \n",
    "    # Printing our found results\n",
    "    model.fit(x_train, y_train)\n",
    "    print('layers:', l,'filters: ', f)\n",
    "    model.evaluate(x_val, y_val)\n",
    "    # Printing empty line for neatness :)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over multiple iterations of running the random search algorithm it is determined that on average 6 layers and 128 filters performs the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers: 6 filters:  128 \n",
      "\n",
      "['Validation Accuracy: 71.39%']\n",
      "\n",
      "['Validation Accuracy: 65.76%']\n",
      "\n",
      "['Validation Accuracy: 70.62%']\n",
      "\n",
      "['Validation Accuracy: 71.14%']\n",
      "\n",
      "['Validation Accuracy: 67.04%']\n",
      "\n",
      "['Validation Accuracy: 71.56%']\n",
      "\n",
      "Ensemble Accuracy:  0.762596071733561\n"
     ]
    }
   ],
   "source": [
    "# This cell performs model averaging, by instantiating multiple models using the best hyperparameters found through the random search algorithm.\n",
    "\n",
    "# Seeding for reproducability\n",
    "random.seed(0)\n",
    "\n",
    "# Defining the number of models variable and constructing a list to store said models \n",
    "n_models = 6\n",
    "model_list = list()\n",
    "\n",
    "# Following block of code creates subsets of data based on the number of models we decide we want to average across\n",
    "nsamples = len(x_train)\n",
    "\n",
    "# Create a data index\n",
    "data_index = list(range(1,nsamples))\n",
    "\n",
    "# Following section simply trains the models with their respective subsets\n",
    "# Printing the best parameters discovered beforehand \n",
    "        \n",
    "l = 6\n",
    "f = 128\n",
    "\n",
    "print('layers:', l,'filters: ', f,'\\n')\n",
    "\n",
    "for i in range(n_models):\n",
    "\n",
    "    model = nn(n_classes= y_train.shape[1], \n",
    "                        n_layers= l,\n",
    "                        n_filters= f,\n",
    "                        filter_size= (3,3),\n",
    "                        stride_length= (2,2),\n",
    "                        d_prob= 0.2,\n",
    "                        epsilon= 0.001)\n",
    "    \n",
    "    # create random index using sampling with replacement\n",
    "    rndx = random.choices(data_index, k=nsamples)\n",
    "\n",
    "    # initialise data set 1\n",
    "    tx = np.zeros([nsamples,98,50,1])\n",
    "    ty = np.zeros([nsamples,12])\n",
    "    \n",
    "    # resample training data with replacement\n",
    "    for j in range(nsamples):\n",
    "        tx[j] = x_train[rndx[j],:,:,:]\n",
    "        ty[j] = y_train[rndx[j],:]\n",
    "\n",
    "    model.fit(tx, ty)\n",
    "    model.evaluate(x_val, y_val)\n",
    "    # Printing empty line for neatness :)\n",
    "    print('')\n",
    "    \n",
    "    model_list.append(model)\n",
    "\n",
    "# Getting model predictions from the appended list of models and storing them in this huge list\n",
    "predictions = [model.model.predict(x_val, verbose=0) for model in model_list]\n",
    "\n",
    "# Summing and converting our predictions into integers \n",
    "prediction_ensemble = np.sum(predictions, axis=0)\n",
    "yhats = np.argmax(prediction_ensemble, axis=1)\n",
    "ys = np.argmax(y_val, axis=1)\n",
    "\n",
    "print('Ensemble Accuracy: ', metrics.accuracy_score(ys, yhats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [09:54<00:00, 29.70s/trial, best loss: -0.7352690100669861]\n",
      "Best parameters determined by Bayesian Optimisation are:  {'filters': 8, 'layers': 6}\n",
      "Validation accuracy with Bayesian parameters is:  {'loss': -0.5046968460083008, 'status': 'ok'}\n"
     ]
    }
   ],
   "source": [
    "# This cell performs Bayesian optimisation using the hyperopt library, reusing code from the neural network class\n",
    "\n",
    "def bayesian_function(params):\n",
    "\n",
    "    '''\n",
    "    Instantiates a sequential neural network based on the input parameters, it is also hard coded to accommodate inputs from this assignment assuming input size of (98,50), pool-size of (2,2), relu activation, same padding, adam optimiser and categorical-cross-entropy loss\n",
    "    \n",
    "    Args: \n",
    "        n_classes (int): The number of classes\n",
    "        n_layers (int): The number of layers, in this context a layer is a convolutional layer + batch normalisation layer + activation + max pool. This does not include the input layer or the dense softmax layer.\n",
    "        n_filters (int): The number of filters added to each convolutional layer\n",
    "        filter_size (tuple (int)): The size of each filter. Should be in format (i, j), i.e. a tuple\n",
    "        stride_length (tuple (int)): The rate at which each filter will convolve over the input. Should be in format (i, j), i.e. a tuple\n",
    "        d_prob (float .00): The dropout probability of the dropout layer. Ideally between (0.00:0.30)\n",
    "        epsilon (float .000): The learning rate of the model\n",
    "    '''\n",
    "    # Initialise the model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Input Layer\n",
    "    model.add(Input(shape=(98,50,1)))\n",
    "\n",
    "    # Convolutional Layers: nothing special here just skips adding a pooling layer at whatever the last convolutional layer is to accommdate for the dropout\n",
    "    for i in range(params['layers']):\n",
    "        model.add(Conv2D(params['filters'], kernel_size = (3,3), padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        if i != params['layers']-1:\n",
    "            model.add(MaxPooling2D(pool_size =(2, 2), strides=(2, 2), padding= 'same'))\n",
    "    \n",
    "    model.add(Dropout((0.2)))\n",
    "    model.add(MaxPooling2D(pool_size =(12, 1), strides=(1, 1), padding = 'same'))\n",
    "    # Final Layer\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(12))\n",
    "    model.add(Softmax())\n",
    "\n",
    "    # Set the optimization options and compile the model\n",
    "    opt = optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "    # Training the model and evaluating on the testing set \n",
    "    model.fit(x_train, y_train, epochs=20, batch_size=16, verbose=0)\n",
    "    loss, accuracy = model.evaluate(x_val, y_val, verbose=0)\n",
    "\n",
    "    return {'loss': -accuracy, 'status': 'ok'}\n",
    "\n",
    "\n",
    "hyperparams = {'layers' : hp.choice('layers', np.arange(1, 10)), 'filters' : hp.choice('filters', np.power(2, np.arange(9)))}\n",
    "\n",
    "bayesian_params = fmin(fn=bayesian_function, space=hyperparams, algo=tpe.suggest, max_evals=20)\n",
    "print('Best parameters determined by Bayesian Optimisation are: ', bayesian_params)\n",
    "                                                                                         \n",
    "bayesian_accuracy = bayesian_function(bayesian_params)\n",
    "print('Validation accuracy with Bayesian parameters is: ', bayesian_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curiously the optimal parameters determined by Bayesian Optimisation are far from the best and the random search algorithm performed better in comparison"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
